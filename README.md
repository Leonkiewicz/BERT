
# BERT from Scratch on MovieDialogs Dataset ğŸ¬ğŸ§   

## Overview  
This project aims to implement **BERT (Bidirectional Encoder Representations from Transformers)** from scratch, using conversational data from the **Cornell Movie-Dialogs Corpus**.  

The primary goal is to train BERT on real dialogue, starting with building a custom tokenizer and gradually moving towards full pretraining tasks like **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.  

---

## Current Progress ğŸš€  

- âœ… Entry data preparation
- âœ… **WordPiece Tokenizer** trained on the MovieDialogs dataset (`bert_tokenizer/`)  
- â³ MLM & NSP not yet implemented   
- âŒ Building BERT architecture   
- âŒ Preparing training routines 

---

## Project Structure  

```
BERT/
â”‚
â”œâ”€â”€ bert_tokenizer/
â”‚   â””â”€â”€ bert-movie_corp-vocab.txt        # Custom tokenizer vocabulary
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ get_data.py
â”‚   â””â”€â”€ preprocessing.py
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ aclImdb/                         # IMDB dataset (optional use)
â”‚   â””â”€â”€ cornell movie-dialogs corpus/    # Main conversational dataset
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ bert.py                          # BERT model implementation
â”‚   â””â”€â”€ layers.py                        # Transformer layers
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                         # Training loop
â”‚   â””â”€â”€ optimizer.py                     # Optimizer setup
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.py                       # Utility functions
â”‚
â”œâ”€â”€ main.py                              # Entry point
â””â”€â”€ early_development.ipynb              # Experimentation notebook
```

---

## How to Use ğŸš€  

### 1. Download the datasets  
Uncomment the `download_files()` line inside `main.py` to automatically download the **Cornell Movie-Dialogs Corpus** and the **IMDB dataset**.  
Alternatively, you can download them manually and place them inside the `datasets/` folder.  

### 2. Preprocess the data  
Run the `main.py` file to preprocess the corpus and train the tokenizer:  
```bash  
python main.py  
```  
This will:  
- Extract question-answer pairs from the movie dialogues.  
- Chunk the data into manageable text files in `datasets/MovieDialogsChunks/`.  
- Train a custom **WordPiece tokenizer** on the movie dialogue data.  

### 3. Train BERT (coming soon)  
Once **MLM** and **NSP** are implemented, you will be able to kick off the BERT pretraining like this:  
```bash  
python training/train.py  
```  

---

## Upcoming Work ğŸ“  

- [ ] Implement **Masked Language Modeling (MLM)**  
- [ ] Implement **Next Sentence Prediction (NSP)**  
- [ ] Integrate pretraining loop  
- [ ] Evaluate embeddings  
- [ ] Fine-tune on downstream tasks (e.g., sentiment analysis on IMDB)  

---

## References  

- [BERT Paper](https://arxiv.org/abs/1810.04805)  
- [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)  
- [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/)  
- [Mastering BERT Model: Building it from Scratch with Pytorch](https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891)

---

